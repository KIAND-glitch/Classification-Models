{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ffa401",
   "metadata": {},
   "source": [
    "# Assignment 2: Classification and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66d47f4",
   "metadata": {},
   "source": [
    "## Overview:\n",
    "For this assignment, you will apply a number of classifiers to various datasets, and\n",
    "explore various evaluation paradigms and analyze the impact of multiple parameters on the performance of the classifiers. You will then answer a number of conceptual\n",
    "questions about the Naive Bayes classifier, K-nearest neighbors, and a number of baselines based on your observations. \n",
    "## Data Sets:\n",
    "In this assignment, you will work with two datasets. These datasets are adapted from a UCI archive public dataset:\n",
    "\n",
    " - **Adult**: You predict whether an adult person earns less than 50K or 50K or more US dollar per year, based on various personal attributes like age or education level. More information can be found<a href=\"https://archive.ics.uci.edu/dataset/2/adult\"> here </a>. \n",
    " - **Student**: You predict a student’s final grade {A+, A, B, C, D, F} based on a number of personal and performance related attributes, such as school, parent’s education level, number of absences, etc. More information can be found<a href=\"https://archive.ics.uci.edu/dataset/697/predict+students+dropout+and+academic+success\"> here </a>. \n",
    " \n",
    "More information about these datasets can be found in `readme.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d2b26164",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import math\n",
    "from collections import defaultdict, Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "afa339cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB,CategoricalNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4d90b111-3c9f-4b4c-a0a6-413000636fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# ignore future warnings \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "468c9fb0-e33e-4fdf-90ee-bfe92daa6e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.3.0.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a1c496",
   "metadata": {},
   "source": [
    "## Question 1. Reading and Pre-processing [1.5 marks] \n",
    "\n",
    "**A)** First, you will read in the data using the `fileName` parameter into a pandas DataFrame. You will also need to input the list of numerical feature names `num_feat` to the function to make your pre-processing easier.\n",
    "\n",
    "**B)** Second, you replace missing values denoted by `?` using the following two strategies: \n",
    "\n",
    "   * <b>Continuous features</b>: For each feature find the <b>average feature value</b> in the dataset \n",
    "   * <b>Categorical features</b>: For each feature find the <b>most frequent value</b> in the dataset  \n",
    "\n",
    "\n",
    "**C)** Third, you will use one-hot encoding to convert all nominal (and ordinal) attributes to numeric. You can achieve this by either using `get_dummies()` from the pandas library or `OneHotEncoder()` from the scikit-learn library. The resulting dataset includes all originally numeric features as well as the one-hot encoded features that are now numeric, call this data `num_dataset`.\n",
    "\n",
    "**D)** Fourth, you will use **equal-width** binning ( 4 bins ) to convert numerical features into categorical. You can achieve this by using `qcut()` from pandas library. The resulting dataset includes all originally categorical features as well as the discretized features that are now categorical, call this data `cat_dataset`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9c33a9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should read a csv file and return two pandas dataframes\n",
    "\n",
    "def preprocess(fileName, numerical_features):\n",
    "    ## read the csv file\n",
    "    data = pd.read_csv(fileName, na_values='?')\n",
    "    data = data.iloc[:, 1:]\n",
    "\n",
    "    features = list(data.iloc[:,:-1].columns)\n",
    "    \n",
    "    # replace missing values with the most frequent for categorical features\n",
    "    categorical_features = [feature for feature in features if feature not in numerical_features]\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        most_freq_value = data[feature].mode()[0]\n",
    "        data[feature].fillna(most_freq_value, inplace=True)\n",
    "    \n",
    "    # replace missing values with the average for numerical features\n",
    "    for feature in numerical_features:\n",
    "        average_value = data[feature].mean()\n",
    "        data[feature].fillna(average_value, inplace=True)\n",
    "    \n",
    "    # convert categorical features to numeric using one-hot encoding\n",
    "    num_dataset = pd.get_dummies(data, columns=categorical_features)\n",
    "    label_column = num_dataset.pop('label')\n",
    "    num_dataset['label'] = label_column\n",
    "\n",
    "    num_bins = 4\n",
    "    cat_dataset = data\n",
    "    \n",
    "    for feature in numerical_features:\n",
    "        width_bin_range = num_dataset[feature].max() - num_dataset[feature].min()\n",
    "        bin_width = width_bin_range / num_bins\n",
    "        width_bin_edges = [num_dataset[feature].min() + i * bin_width for i in range(num_bins + 1)]\n",
    "    \n",
    "        cat_dataset[feature] = pd.cut(num_dataset[feature], bins=width_bin_edges, include_lowest=True, labels=False)\n",
    "    \n",
    "    return cat_dataset, num_dataset\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "continental-administration",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## list of numeric features for adult dataset\n",
    "adult_num = ['Age','fnlwgt','Education-num','Capital-gain','Capital-loss','Hours-per-week']\n",
    "\n",
    "## generate the categorical and numerical adult datasets\n",
    "adult_cat_dataset,adult_num_dataset = preprocess(\"datasets/adult.csv\",adult_num)\n",
    "\n",
    "## generate the categorical and numerical student datasets\n",
    "student_cat_dataset,student_num_dataset = preprocess(\"datasets/student.csv\",[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b0a55928-9e77-4049-bc26-13c8ab20b860",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_dataset = pd.read_csv(\"datasets/student.csv\", na_values='?')\n",
    "adult_dataset = pd.read_csv(\"datasets/adult.csv\", na_values='?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-joint",
   "metadata": {},
   "source": [
    "#### Question 2 . Baseline methods and Discussion [4.5 marks]\n",
    "**A)** For 10 rounds, use `train_test_split` to divide the processed `cat_dataset` into 80% train, 20% test . Set the `random_state` equal to the loop counter. For example in the loop\n",
    "``` python \n",
    "for i in range(10):\n",
    "```\n",
    "make `random_state` equal to `i`. \n",
    "Use the splitted datasets to train and test the following models: **[1 mark]**\n",
    "\n",
    "- Zero-R\n",
    "- One-R\n",
    "- Weighted Random \n",
    "\n",
    "Report the average accuracy over the 10 runs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "south-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Zero-R baseline\n",
    "zero_r = DummyClassifier(strategy='most_frequent')\n",
    "\n",
    "# One-R baseline\n",
    "def find_best_feature(X, y):\n",
    "    best_feature = None\n",
    "    best_error = float('inf')\n",
    "    \n",
    "    for feature in range(X.shape[1]):\n",
    "        unique_values = np.unique(X.iloc[:, feature])\n",
    "        rules = {value: y[X.iloc[:, feature] == value].value_counts().idxmax() for value in unique_values}\n",
    "        \n",
    "        error = np.sum(y != np.array([rules[x[feature]] for x in X.values]))\n",
    "        \n",
    "        if error < best_error:\n",
    "            best_feature, best_rules, best_error = feature, rules, error\n",
    "            \n",
    "    return best_feature, best_rules\n",
    "\n",
    "def one_r_train(X_train, y_train):\n",
    "    best_feature, best_rules = find_best_feature(X_train, y_train)\n",
    "    return best_feature, best_rules\n",
    "\n",
    "def one_r_predict(X_test, best_feature, best_rules, majority_class):\n",
    "    predictions = [best_rules.get(x[best_feature], majority_class) for x in X_test.values]\n",
    "    return np.array(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "07e3d435-af7a-4bd4-b481-f22b1c7550a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Baseline results:\n",
      "Accuracy of ZeroR: 0.76\n",
      "Accuracy of One-R: 0.77\n",
      "Accuracy of Weighted Random: 0.63\n",
      "Student Dataset Baseline results:\n",
      "Accuracy of ZeroR: 0.3\n",
      "Accuracy of One-R: 0.31\n",
      "Accuracy of Weighted Random: 0.23\n"
     ]
    }
   ],
   "source": [
    "def baselines(cat_dataset):\n",
    "\n",
    "    ZeroR_Acc_1 = []\n",
    "    OneR_Acc_1 = []\n",
    "    WRand_Acc_1 = []\n",
    "\n",
    "    ## your code here\n",
    "    x = cat_dataset.iloc[:,:-1]\n",
    "    y = cat_dataset.iloc[:,-1]\n",
    "    \n",
    "    for i in range(10):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=i)\n",
    "\n",
    "        # Zero-R\n",
    "        zero_r.fit(x_train, y_train)\n",
    "        y_pred = zero_r.predict(x_test)\n",
    "        ZeroR_Acc_1.append(accuracy_score(y_test, y_pred))\n",
    "\n",
    "        # One-R\n",
    "        best_feature, best_rules = one_r_train(x_train, y_train)\n",
    "        majority_class = Counter(y_train).most_common(1)[0][0]\n",
    "        one_r_predictions = one_r_predict(x_test, best_feature, best_rules, majority_class)\n",
    "        OneR_Acc_1.append(accuracy_score(y_test, one_r_predictions))\n",
    "\n",
    "        # Weighted Random\n",
    "        values, counts = np.unique(y_train, return_counts=True)\n",
    "        weights = counts / (sum(counts))\n",
    "        weighted_random_predictions = random.choices(values, weights=weights, k=y_test.shape[0])\n",
    "        WRand_Acc_1.append(accuracy_score(y_test, weighted_random_predictions))\n",
    "    \n",
    "    print(\"Accuracy of ZeroR:\", np.mean(ZeroR_Acc_1).round(2))\n",
    "    print(\"Accuracy of One-R:\", np.mean(OneR_Acc_1).round(2))\n",
    "    print(\"Accuracy of Weighted Random:\", np.mean(WRand_Acc_1).round(2))\n",
    "    \n",
    "##Adult Dataset and Student Dataset results: \n",
    "print(\"Adult Dataset Baseline results:\")\n",
    "baselines(adult_cat_dataset)\n",
    "\n",
    "print(\"Student Dataset Baseline results:\")\n",
    "baselines(student_cat_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c394c5",
   "metadata": {},
   "source": [
    "**B)** After comparing the performance of the different models on the classification task, please comment on any differences or lack of differences you observe between the baseline models and the datasets. **[1.5 marks]**</br>\n",
    "*NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "7bbd52a3-442d-4255-8371-8db8f60a0ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Classifier Reports:\n",
      "Zero-R Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        >50K       0.77      1.00      0.87       385\n",
      "       <=50K       0.00      0.00      0.00       115\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.39      0.50      0.44       500\n",
      "weighted avg       0.59      0.77      0.67       500\n",
      "\n",
      "One-R Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        >50K       0.77      1.00      0.87       385\n",
      "       <=50K       1.00      0.02      0.03       115\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.89      0.51      0.45       500\n",
      "weighted avg       0.83      0.77      0.68       500\n",
      "\n",
      "Weighted Random Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        >50K       0.78      0.76      0.77       385\n",
      "       <=50K       0.25      0.27      0.26       115\n",
      "\n",
      "    accuracy                           0.65       500\n",
      "   macro avg       0.51      0.52      0.51       500\n",
      "weighted avg       0.66      0.65      0.65       500\n",
      "\n",
      "Student Dataset Classifier Reports:\n",
      "Zero-R Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.00      0.00      0.00        11\n",
      "           F       0.00      0.00      0.00         2\n",
      "           A       0.00      0.00      0.00        24\n",
      "           D       0.00      0.00      0.00        40\n",
      "           B       0.29      1.00      0.45        38\n",
      "          A+       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.29       130\n",
      "   macro avg       0.05      0.17      0.08       130\n",
      "weighted avg       0.09      0.29      0.13       130\n",
      "\n",
      "One-R Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.00      0.00      0.00        11\n",
      "           F       0.00      0.00      0.00         2\n",
      "           A       0.16      0.21      0.18        24\n",
      "           D       0.00      0.00      0.00        40\n",
      "           B       0.33      0.84      0.47        38\n",
      "          A+       0.00      0.00      0.00        15\n",
      "\n",
      "    accuracy                           0.28       130\n",
      "   macro avg       0.08      0.18      0.11       130\n",
      "weighted avg       0.12      0.28      0.17       130\n",
      "\n",
      "Weighted Random Classifier Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           C       0.00      0.00      0.00        11\n",
      "           F       0.00      0.00      0.00         2\n",
      "           A       0.14      0.12      0.13        24\n",
      "           D       0.19      0.15      0.17        40\n",
      "           B       0.38      0.34      0.36        38\n",
      "          A+       0.09      0.13      0.11        15\n",
      "\n",
      "    accuracy                           0.18       130\n",
      "   macro avg       0.13      0.13      0.13       130\n",
      "weighted avg       0.21      0.18      0.19       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_baselines(cat_dataset):\n",
    "    def evaluate_one_round(y_true, y_pred, target_names):\n",
    "        report = classification_report(y_true, y_pred, target_names=target_names, zero_division=0)\n",
    "        print(report)\n",
    "    \n",
    "    x = cat_dataset.iloc[:,:-1]\n",
    "    y = cat_dataset.iloc[:,-1]\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0)\n",
    "\n",
    "    # Zero-R\n",
    "    zero_r = DummyClassifier(strategy='most_frequent')\n",
    "    zero_r.fit(x_train, y_train)\n",
    "\n",
    "    # One-R\n",
    "    best_feature, best_rules = one_r_train(x_train, y_train)\n",
    "    majority_class = Counter(y_train).most_common(1)[0][0]\n",
    "\n",
    "    # Weighted Random\n",
    "    values, counts = np.unique(y_train, return_counts=True)\n",
    "    weights = counts / (sum(counts))\n",
    "\n",
    "    # Predictions for each model\n",
    "    zero_r_predictions = zero_r.predict(x_test)\n",
    "    one_r_predictions = one_r_predict(x_test, best_feature, best_rules, majority_class)\n",
    "    weighted_random_predictions = random.choices(values, weights=weights, k=y_test.shape[0])\n",
    "\n",
    "    unique_class_labels = y_train.unique()\n",
    "\n",
    "    # Evaluate each model's performance using classification_report\n",
    "    print(\"Zero-R Classifier Report:\")\n",
    "    evaluate_one_round(y_test, zero_r_predictions, unique_class_labels)\n",
    "\n",
    "    print(\"One-R Classifier Report:\")\n",
    "    evaluate_one_round(y_test, one_r_predictions, unique_class_labels)\n",
    "\n",
    "    print(\"Weighted Random Classifier Report:\")\n",
    "    evaluate_one_round(y_test, weighted_random_predictions, unique_class_labels)\n",
    "\n",
    "print(\"Adult Dataset Classifier Reports:\")\n",
    "evaluate_baselines(adult_cat_dataset)\n",
    "print(\"Student Dataset Classifier Reports:\")\n",
    "evaluate_baselines(student_cat_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c21efb-193a-495f-96dd-ac4f40f97690",
   "metadata": {},
   "source": [
    "Adult Dataset:\n",
    "For the Zero-R classifier, it predicts the majority class (>50K) for all instances, resulting in high precision, recall and f1 score for the >50K class, but low values for the '<=50K'. This is expected since it's always predicting the dominant class.\n",
    "The One-R classifier performs similarly to the Zero-R classifier in terms of precision and recall for the >50K class, with improvement in predictions for '<=50K' at is creates rules based on a single feature to differentiate between classes.\n",
    "The Weighted Random classifier seems to make more balanced predictions, with similar precision and recall values for both classes. However, its overall accuracy is lower (0.65) compared to the other classifiers.\n",
    "\n",
    "Student Dataset:\n",
    "For the Zero-R classifier, it's again predicting the majority class (B), leading to high precision, recall and f1 score for that class. Other classes have very low precision and recall values due to this bias, with the models accuracy being 29%.\n",
    "The One-R classifier shows an improvement over Zero-R for classes like B and D in terms of precision, recall and f1 score, but struggles with classes with fewer instances like F and A+ due to the simplicity of the rules.\n",
    "The Weighted Random classifier, again has balanced predictions with similar precision and recall values for most classes. However, its overall accuracy is the lowest (0.18) among the three classifiers.\n",
    "\n",
    "\n",
    "For both datasets, the baseline models (Zero-R, One-R, and Weighted Random) generally achieve low performance in terms of precision, recall, and F1-score for individual classes compared to more advanced models.\n",
    "There is a lack of significant differences between the baseline models in terms of their performance within each dataset. This could be due to the simplicity of the models and the underlying patterns in the data that these models are unable to capture effectively.\n",
    "The Weighted Random classifier tends to have more balanced predictions for class labels in both datasets compared to the Zero-R and One-R classifiers. However, its overall accuracy is generally lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-worker",
   "metadata": {},
   "source": [
    "**C)** Update your code for One-R so that you can inspect the feature that is most often selected in the 10 rounds of training and testing for each dataset. Write the classification rule using the best feature and its values for each dataset. **[1 mark]**</br>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "74131270-3157-4c23-84ef-0dae97a003ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Baseline results:\n",
      "Accuracy of One-R: 0.77\n",
      "Most often selected feature: Capital-gain\n",
      "Classification rule: {0: '<=50K', 1: '>50K', 3: '>50K'}\n",
      "Student Dataset Baseline results:\n",
      "Accuracy of One-R: 0.31\n",
      "Most often selected feature: Fedu\n",
      "Classification rule: {'high': 'C', 'low': 'D', 'mid': 'D', 'none': 'D'}\n"
     ]
    }
   ],
   "source": [
    "def one_r_train_with_tracking(X_train, y_train, selected_features):\n",
    "    best_feature, best_rules = find_best_feature(X_train, y_train)\n",
    "    selected_features[best_feature] += 1\n",
    "    return best_feature, best_rules\n",
    "\n",
    "def baselines_with_feature_tracking(cat_dataset):\n",
    "    \n",
    "    selected_features = defaultdict(int)\n",
    "    \n",
    "    x = cat_dataset.iloc[:,:-1]\n",
    "    y = cat_dataset.iloc[:,-1]\n",
    "    OneR_Acc_1 = []\n",
    "\n",
    "    \n",
    "    for i in range(10):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=i)\n",
    "\n",
    "        # One-R with feature tracking\n",
    "        best_feature, best_rules = one_r_train_with_tracking(x_train, y_train, selected_features)\n",
    "        majority_class = Counter(y_train).most_common(1)[0][0]\n",
    "        one_r_predictions = one_r_predict(x_test, best_feature, best_rules, majority_class)\n",
    "        OneR_Acc_1.append(accuracy_score(y_test, one_r_predictions))\n",
    "    \n",
    "    print(\"Accuracy of One-R:\", np.mean(OneR_Acc_1).round(2))\n",
    "    \n",
    "    # Identify the most often selected feature and its frequency\n",
    "    most_selected_feature = max(selected_features, key=selected_features.get)\n",
    "    print(\"Most often selected feature:\", x.columns[most_selected_feature])\n",
    "    \n",
    "    # Write classification rule using the best feature and its values\n",
    "    unique_values = np.unique(x_train.iloc[:, most_selected_feature])\n",
    "    rule = {value: best_rules[value] for value in unique_values}\n",
    "    print(\"Classification rule:\", rule)\n",
    "\n",
    "##Adult Dataset and Student Dataset results: \n",
    "print(\"Adult Dataset Baseline results:\")\n",
    "baselines_with_feature_tracking(adult_cat_dataset)\n",
    "\n",
    "print(\"Student Dataset Baseline results:\")\n",
    "baselines_with_feature_tracking(student_cat_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-tiffany",
   "metadata": {},
   "source": [
    "**D)** For weighted random baseline applied to Adult dataset, what would the error rate converge to (Write a formula based on the prior probability of the dominant class, named `prior`, and the fraction of test samples belonging to the dominant class, `fraction`)? **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surrounded-convention",
   "metadata": {},
   "source": [
    "The error rate for the weighted random baseline applied to the Adult dataset would converge to the error rate of the dominant class. The formula to calculate the error rate based on the prior probability of the dominant class (prior) and the fraction of test samples belonging to the dominant class (fraction) is given by:\n",
    "\n",
    "\n",
    "Error Rate = (1 - prior) * fraction + prior * (1 - fraction)\n",
    "\n",
    "\n",
    "\n",
    "Explanation:\n",
    "\n",
    "The first term (1 - prior) * fraction represents the error rate when the model predicts the non-dominant class incorrectly (1 - prior) and it happens for a fraction of the test samples (fraction).\n",
    "\n",
    "The second term prior * (1 - fraction) represents the error rate when the model predicts the dominant class incorrectly (prior) and it happens for the remaining fraction of the test samples (1 - fraction).\n",
    "\n",
    "This formula takes into account the prior probability of the dominant class and the proportion of samples in the test set that belong to the dominant class. It gives an estimate of the error rate that the weighted random baseline would converge to when applied to the Adult dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ec6a7b",
   "metadata": {},
   "source": [
    "## Question 3. Naive Bayes models [5 marks]\n",
    "\n",
    "**A)** Divide the `num_dataset` and `cat_dataset` into 80% train and 20% test splits for 10 rounds, set the `random_state` equal to the loop counter. Then, train and test the following models:\n",
    "\n",
    "- Gaussian Naive Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "- Categorical Naive Bayes \n",
    "\n",
    "You must use the input data that you believe is best suited for each model. Finally, report the average accuracy of the NB models over the 10 runs. **[1 mark]**\n",
    "\n",
    "**Note: You may need to change your input format to be able to use sklearn's CategoricalNB.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e4680dd4-183c-4066-a5d1-e08d5186e69e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset NB results:\n",
      "Accuracy of GNB: 0.8\n",
      "Accuracy of BNB: 0.79\n",
      "Accuracy of CNB: 0.8\n",
      "\n",
      "\n",
      "Student Dataset NB results:\n",
      "Accuracy of GNB: 0.17\n",
      "Accuracy of BNB: 0.32\n",
      "Accuracy of CNB: 0.34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, CategoricalNB\n",
    "import numpy as np\n",
    "\n",
    "def NB_models(num_dataset, cat_dataset):\n",
    "    GNB_Acc_1 = []\n",
    "    BNB_Acc_1 = []\n",
    "    CNB_Acc_1 = []\n",
    "\n",
    "    cat_dataset_X = cat_dataset.drop('label', axis=1)\n",
    "    categorical_columns = cat_dataset_X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    encoder = OrdinalEncoder()\n",
    "    cat_dataset_X_encoded = cat_dataset_X.copy()\n",
    "    cat_dataset_X_encoded[categorical_columns] = encoder.fit_transform(cat_dataset_X_encoded[categorical_columns])\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    bnb = BernoulliNB()\n",
    "    min_categories = cat_dataset_X_encoded.nunique()\n",
    "    cnb = CategoricalNB(min_categories=min_categories)\n",
    "    \n",
    "    for i in range(10):\n",
    "\n",
    "        X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(cat_dataset_X_encoded, cat_dataset['label'], test_size=0.2, random_state=i)\n",
    "        X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(num_dataset.iloc[:, :-1], num_dataset['label'], test_size=0.2, random_state=i)\n",
    "\n",
    "        # Gaussian Naive Bayes\n",
    "        gnb.fit(X_train_num, y_train_num)\n",
    "        y_pred = gnb.predict(X_test_num)\n",
    "        GNB_Acc_1.append(accuracy_score(y_test_num, y_pred))\n",
    "\n",
    "        # Bernoulli Naive Bayes\n",
    "        bnb.fit(X_train_num, y_train_num)\n",
    "        y_pred = bnb.predict(X_test_num)\n",
    "        BNB_Acc_1.append(accuracy_score(y_test_num, y_pred))\n",
    "\n",
    "        # Categorical Naive Bayes\n",
    "        cnb.fit(X_train_cat, y_train_cat)\n",
    "        y_pred = cnb.predict(X_test_cat)\n",
    "        CNB_Acc_1.append(accuracy_score(y_test_cat, y_pred))\n",
    "\n",
    "    print(\"Accuracy of GNB:\", np.mean(GNB_Acc_1).round(2))\n",
    "    print(\"Accuracy of BNB:\", np.mean(BNB_Acc_1).round(2))\n",
    "    print(\"Accuracy of CNB:\", np.mean(CNB_Acc_1).round(2))\n",
    "\n",
    "print(\"Adult Dataset NB results:\")\n",
    "NB_models(adult_num_dataset, adult_cat_dataset)\n",
    "print(\"\\n\")\n",
    "print(\"Student Dataset NB results:\")\n",
    "NB_models(student_num_dataset, student_cat_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "cc563195-c2da-4bcb-b970-5fcbd98589e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset NB results:\n",
      "Classification Report for Gaussian NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.82      0.95      0.88       385\n",
      "        >50K       0.64      0.32      0.43       115\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.73      0.63      0.65       500\n",
      "weighted avg       0.78      0.80      0.78       500\n",
      "\n",
      "Classification Report for Bernoulli NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.81      0.96      0.88       385\n",
      "        >50K       0.66      0.25      0.36       115\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.74      0.61      0.62       500\n",
      "weighted avg       0.78      0.80      0.76       500\n",
      "\n",
      "Classification Report for Categorical NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.83      0.92      0.88       385\n",
      "        >50K       0.60      0.37      0.46       115\n",
      "\n",
      "    accuracy                           0.80       500\n",
      "   macro avg       0.71      0.65      0.67       500\n",
      "weighted avg       0.78      0.80      0.78       500\n",
      "\n",
      "Student Dataset NB results:\n",
      "Classification Report for Gaussian NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.11      0.18      0.14        11\n",
      "          A+       0.01      0.50      0.03         2\n",
      "           B       0.39      0.29      0.33        24\n",
      "           C       0.00      0.00      0.00        40\n",
      "           D       0.50      0.03      0.05        38\n",
      "           F       0.29      0.27      0.28        15\n",
      "\n",
      "    accuracy                           0.12       130\n",
      "   macro avg       0.22      0.21      0.14       130\n",
      "weighted avg       0.26      0.12      0.12       130\n",
      "\n",
      "Classification Report for Bernoulli NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.21      0.36      0.27        11\n",
      "          A+       0.00      0.00      0.00         2\n",
      "           B       0.17      0.17      0.17        24\n",
      "           C       0.33      0.28      0.30        40\n",
      "           D       0.38      0.32      0.34        38\n",
      "           F       0.15      0.20      0.17        15\n",
      "\n",
      "    accuracy                           0.26       130\n",
      "   macro avg       0.21      0.22      0.21       130\n",
      "weighted avg       0.28      0.26      0.27       130\n",
      "\n",
      "Classification Report for Categorical NB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.20      0.27      0.23        11\n",
      "          A+       0.00      0.00      0.00         2\n",
      "           B       0.20      0.12      0.15        24\n",
      "           C       0.33      0.33      0.33        40\n",
      "           D       0.37      0.45      0.40        38\n",
      "           F       0.15      0.13      0.14        15\n",
      "\n",
      "    accuracy                           0.29       130\n",
      "   macro avg       0.21      0.22      0.21       130\n",
      "weighted avg       0.28      0.29      0.28       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def NB_models_classification(num_dataset, cat_dataset):\n",
    "\n",
    "    cat_dataset_X = cat_dataset.drop('label', axis=1)\n",
    "    categorical_columns = cat_dataset_X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "    encoder = OrdinalEncoder()\n",
    "    cat_dataset_X_encoded = cat_dataset_X.copy()\n",
    "    cat_dataset_X_encoded[categorical_columns] = encoder.fit_transform(cat_dataset_X_encoded[categorical_columns])\n",
    "\n",
    "    gnb = GaussianNB()\n",
    "    bnb = BernoulliNB()\n",
    "    min_categories = cat_dataset_X_encoded.nunique()\n",
    "    cnb = CategoricalNB(min_categories=min_categories)\n",
    "\n",
    "    X_train_cat, X_test_cat, y_train_cat, y_test_cat = train_test_split(cat_dataset_X_encoded, cat_dataset['label'], test_size=0.2, random_state=0)\n",
    "    X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(num_dataset.iloc[:, :-1], num_dataset['label'], test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "    # Gaussian Naive Bayes\n",
    "    gnb.fit(X_train_num, y_train_num)\n",
    "    y_pred_gnb = gnb.predict(X_test_num)\n",
    "    print(\"Classification Report for Gaussian NB:\")\n",
    "    print(classification_report(y_test_num, y_pred_gnb))\n",
    "\n",
    "    # Bernoulli Naive Bayes\n",
    "    bnb.fit(X_train_num, y_train_num)\n",
    "    y_pred_bnb = bnb.predict(X_test_num)\n",
    "    print(\"Classification Report for Bernoulli NB:\")\n",
    "    print(classification_report(y_test_num, y_pred_bnb))\n",
    "\n",
    "    # Categorical Naive Bayes\n",
    "    cnb.fit(X_train_cat, y_train_cat)\n",
    "    y_pred_cnb = cnb.predict(X_test_cat)\n",
    "    print(\"Classification Report for Categorical NB:\")\n",
    "    print(classification_report(y_test_cat, y_pred_cnb))\n",
    "\n",
    "# Call the function for the Adult and Student datasets\n",
    "print(\"Adult Dataset NB results:\")\n",
    "NB_models_classification(adult_num_dataset, adult_cat_dataset)\n",
    "\n",
    "print(\"Student Dataset NB results:\")\n",
    "NB_models_classification(student_num_dataset, student_cat_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25586ac4-d7a4-4780-a128-1628955bae88",
   "metadata": {},
   "source": [
    "**B)** How does the performance of the Naive Bayes classifiers compare against your baseline models for each dataset? **[1 mark]** Please comment on any differences you observe between the baseline models and the NB models in the context of the two datasets.</br> *NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "second-criterion",
   "metadata": {},
   "source": [
    "Adult Dataset:\n",
    "\n",
    "Zero-R Classifier: This classifier predicts the majority class (\">50K\") for all instances. It achieves an accuracy of 0.77, but its F1-score for the \"<=50K\" class is extremely low (0.00).\n",
    "One-R Classifier: This classifier uses a simple rule-based approach and achieves similar accuracy (0.77) but has a low F1-score for the \"<=50K\" class (0.03).\n",
    "Weighted Random Classifier: This random classifier performs the worst with an accuracy of 0.65.\n",
    "\n",
    "Gaussian NB Classifier: The Gaussian NB model performs notably better than the baselines, achieving an accuracy of 0.80. It provides improved F1-scores for both classes, especially for the \">50K\" class (0.43).\n",
    "Bernoulli NB Classifier: The Bernoulli NB model also achieves an accuracy of 0.80 and improves the F1-scores for both classes compared to the baselines.\n",
    "Categorical NB Classifier: The Categorical NB model, similar to Gaussian and Bernoulli NB, attains an accuracy of 0.80 and enhances the F1-scores for both classes.\n",
    "\n",
    "\n",
    "Student Dataset:\n",
    "\n",
    "Zero-R Classifier: This classifier predicts the majority class (\"B\") for all instances and achieves an accuracy of 0.29.\n",
    "One-R Classifier: This classifier, like Zero-R, performs poorly with an accuracy of 0.28.\n",
    "Weighted Random Classifier: The random classifier performs slightly better with an accuracy of 0.18.\n",
    "\n",
    "Gaussian NB Classifier: The Gaussian NB model significantly improves the accuracy to 0.12 compared to the baselines, but the F1-scores remain relatively low for most classes.\n",
    "Bernoulli NB Classifier: The Bernoulli NB model also shows improved accuracy (0.26), but like Gaussian NB, the F1-scores are relatively low for many classes.\n",
    "Categorical NB Classifier: Similar to Gaussian and Bernoulli NB, the Categorical NB model achieves an accuracy of 0.29, with limited improvement in F1-scores.\n",
    "\n",
    "Differences Observed:\n",
    "\n",
    "In both datasets, the Naive Bayes models consistently outperform the baselines in terms of accuracy.\n",
    "The Naive Bayes models generally provide better F1-scores for both classes in the Adult dataset indicating improved precision and recall.\n",
    "The Student Dataset presents a more challenging problem, and even the Naive Bayes models struggle to achieve high F1-scores for most classes. However, they still offer some improvement over the baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-information",
   "metadata": {},
   "source": [
    "**C)** The three Naive Bayes (NB) classifiers lead to different performances. Which of these NB classifiers performs best for each dataset, and why do you think it is the case? **[1 mark]** *NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.*\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f432d23-db12-44e9-b083-e7d1969e8d9a",
   "metadata": {},
   "source": [
    "Adult Dataset:\n",
    "\n",
    "Gaussian NB Classifier performs the best for the Adult Dataset. It achieves an accuracy of 0.80, the highest among the NB models. It also provides reasonably balanced F1-scores for both classes (\"<=50K\" and \">50K\").\n",
    "Bernoulli NB Classifier also performs well with an accuracy of 0.80 but has slightly lower F1-scores compared to Gaussian NB.\n",
    "Categorical NB Classifier has an accuracy of 0.80, but has slightly lower F1-scores than Gaussian and Bernoulli NB for both classes.\n",
    "\n",
    "Student Dataset:\n",
    "\n",
    "Gaussian NB Classifier has an accuracy of 0.12 and performs poorly with low precision, recall, and F1-scores for most classes, indicating challenges in correctly classifying various categories. Bernoulli NB Classifier performs better with an accuracy of 0.26 but still faces difficulties in accurately classifying several classes. Categorical NB Classifier has an accuracy of 0.29 but struggles to accurately classify multiple classes, leading to moderate precision, recall, and F1-scores for various categories.\n",
    "\n",
    "Some possible reasons for this are:\n",
    "\n",
    "Feature Independence Assumption:\n",
    "All NB classifiers make the naive assumption of feature independence, which may or may not hold true in the real-world data. The suitability of this assumption can impact performance.\n",
    "\n",
    "Data Complexity:\n",
    "The complexity and distribution of data in the Student Dataset are quite different from the Adult Dataset. The Student Dataset appears to be more challenging, with a larger number of classes and potentially non-linear relationships between features and the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-check",
   "metadata": {},
   "source": [
    "**D)** The Gaussian Naive Bayes classifier makes two fundamental assumptions: (1) about the distribution of $P(x_j|c_i)$ and (2) about the (conditional) dependency structure between features.\n",
    "Explain both assumptions, and discuss whether these assumptions are always true for the\n",
    "numeric attributes in the Adult dataset. If applicable, identify some cases where the assumptions are violated. **[2 marks]**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quarterly-luther",
   "metadata": {},
   "source": [
    "The Gaussian Naive Bayes (GNB) classifier makes two fundamental assumptions:\n",
    "\n",
    "Distribution of P(xj|ci): GNB assumes that the numerical attributes (xj) of each class (ci) follow a Gaussian (normal) distribution.\n",
    "\n",
    "Conditional Dependency Structure between features: GNB assumes that the features are conditionally independent given the class label, which means that the value of any one feature does not depend on the values of other features, given the class label. \n",
    "\n",
    "P(x1, x2, ..., xn|ci) = P(x1|ci) * P(x2|ci) * ... * P(xn|ci).\n",
    "\n",
    "\n",
    "The assumptions of GNB may not always hold true for the numeric attributes in the Adult dataset:\n",
    "\n",
    "Distribution Assumption (Gaussian Distribution):While some features might have distributions close to Gaussian, others could have more complex or skewed distributions.\n",
    "For example, attributes like \"capital-gain\" and \"capital-loss\" are likely to have distributions with most values being zeros or near zero and a few extremely high values. \n",
    "\n",
    "\n",
    "Conditional Independence Assumption:The assumption of conditional independence might be violated in cases where there are strong correlations or relationships between numeric attributes. \n",
    "For example, Income (\"income\") might be strongly correlated with education level (\"education-num\") or occupation (\"occupation\").\n",
    "\n",
    "\n",
    "In summary, while GNB's assumptions are simplifications that can work well for certain datasets, the numeric attributes in the Adult dataset include features violate these assumptions, and therefore GNB may not be suitable for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de4822d",
   "metadata": {},
   "source": [
    "## Question 4. K-Nearest Neighbor [3 marks] \n",
    "**A)** Divide the `num_dataset` into 80% train and 20% test splits for 10 rounds, set the `random_state` equal to the loop counter. Then, train and test the following models:\n",
    "\n",
    "- 6 K-Nearest Neighbor models with Euclidean distance using the following parameters:\n",
    "\n",
    "    - with K values of 1,5, and 10\n",
    "    \n",
    "    - using inverse distance weighting and majority voting \n",
    "\n",
    "Finally, report the average accuracy of the KNN models over the 10 rounds. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0859a139",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset KNN results:\n",
      "Accuracy of weighted KNN(1): 0.7\n",
      "Accuracy of weighted KNN(5): 0.74\n",
      "Accuracy of weighted KNN(10): 0.76\n",
      "Accuracy of KNN(1): 0.7\n",
      "Accuracy of KNN(5): 0.78\n",
      "Accuracy of KNN(10): 0.79\n",
      "\n",
      "\n",
      "Student Dataset KNN results:\n",
      "Accuracy of weighted KNN(1): 0.26\n",
      "Accuracy of weighted KNN(5): 0.28\n",
      "Accuracy of weighted KNN(10): 0.29\n",
      "Accuracy of KNN(1): 0.26\n",
      "Accuracy of KNN(5): 0.27\n",
      "Accuracy of KNN(10): 0.28\n"
     ]
    }
   ],
   "source": [
    "def KNNs(num_dataset):\n",
    "    KNN1_Acc_1_weighted = []\n",
    "    KNN5_Acc_1_weighted = []\n",
    "    KNN10_Acc_1_weighted = []\n",
    "    KNN1_Acc_1_majority = []\n",
    "    KNN5_Acc_1_majority = []\n",
    "    KNN10_Acc_1_majority = []\n",
    "\n",
    "\n",
    "    ## your code here\n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(num_dataset.iloc[:,:-1], num_dataset.iloc[:,-1], test_size=0.2, random_state=i)\n",
    "        \n",
    "        for k in [1, 5, 10]:\n",
    "            # Weighted KNN with inverse distance weighting\n",
    "            knn_weighted = KNeighborsClassifier(n_neighbors=k, weights='distance', metric='euclidean')\n",
    "            knn_weighted.fit(X_train.values, y_train)\n",
    "            y_pred = knn_weighted.predict(X_test.values)\n",
    "            acc_weighted = accuracy_score(y_test, y_pred)\n",
    "            \n",
    "            # KNN with majority voting\n",
    "            knn_majority = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='euclidean')\n",
    "            knn_majority.fit(X_train.values, y_train)\n",
    "            y_pred = knn_majority.predict(X_test.values)\n",
    "            acc_majority = accuracy_score(y_test, y_pred)\n",
    "\n",
    "            if k == 1:\n",
    "                KNN1_Acc_1_weighted.append(acc_weighted)\n",
    "                KNN1_Acc_1_majority.append(acc_majority)\n",
    "            elif k == 5:\n",
    "                KNN5_Acc_1_weighted.append(acc_weighted)\n",
    "                KNN5_Acc_1_majority.append(acc_majority)\n",
    "            elif k == 10:\n",
    "                KNN10_Acc_1_weighted.append(acc_weighted)\n",
    "                KNN10_Acc_1_majority.append(acc_majority)\n",
    "\n",
    "            \n",
    "    print(\"Accuracy of weighted KNN(1):\", np.mean(KNN1_Acc_1_weighted).round(2))\n",
    "    print(\"Accuracy of weighted KNN(5):\", np.mean(KNN5_Acc_1_weighted).round(2))\n",
    "    print(\"Accuracy of weighted KNN(10):\", np.mean(KNN10_Acc_1_weighted).round(2))\n",
    "    print(\"Accuracy of KNN(1):\", np.mean(KNN1_Acc_1_majority).round(2))\n",
    "    print(\"Accuracy of KNN(5):\", np.mean(KNN5_Acc_1_majority).round(2))\n",
    "    print(\"Accuracy of KNN(10):\", np.mean(KNN10_Acc_1_majority).round(2))\n",
    "    \n",
    "    \n",
    "##Adult Dataset and Student Dataset results: \n",
    "\n",
    "print(\"Adult Dataset KNN results:\")\n",
    "KNNs(adult_num_dataset)\n",
    "print(\"\\n\")\n",
    "print(\"Student Dataset KNN results:\")\n",
    "KNNs(student_num_dataset)\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8357cd59",
   "metadata": {},
   "source": [
    "**B)** Compare the results of the weighted and majority KNN models (for each value of K) and explain any differences you observe for each dataset in terms of the voting strategy and the number of nearest neighbors. **[1 marks]**</br> *NOTE: You may need to compare other performance metrics of these models, such as precision and recall of each class label, to gain a better understanding of their performance. You can use the `classification_report` from `sklearn.metrics` for this matter and check the performance of the classifiers for one round.* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-percentage",
   "metadata": {},
   "source": [
    "*Answer Here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "3995e378-71b3-41ae-898c-e31d0715acba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for Weighted KNN (K=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.79      0.81      0.80       381\n",
      "        >50K       0.35      0.33      0.34       119\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.69      0.70      0.69       500\n",
      "\n",
      "Classification Report for Majority KNN (K=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.79      0.81      0.80       381\n",
      "        >50K       0.35      0.33      0.34       119\n",
      "\n",
      "    accuracy                           0.70       500\n",
      "   macro avg       0.57      0.57      0.57       500\n",
      "weighted avg       0.69      0.70      0.69       500\n",
      "\n",
      "Classification Report for Weighted KNN (K=5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.79      0.88      0.83       381\n",
      "        >50K       0.39      0.25      0.31       119\n",
      "\n",
      "    accuracy                           0.73       500\n",
      "   macro avg       0.59      0.56      0.57       500\n",
      "weighted avg       0.69      0.73      0.71       500\n",
      "\n",
      "Classification Report for Majority KNN (K=5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.79      0.96      0.86       381\n",
      "        >50K       0.57      0.18      0.27       119\n",
      "\n",
      "    accuracy                           0.77       500\n",
      "   macro avg       0.68      0.57      0.57       500\n",
      "weighted avg       0.74      0.77      0.72       500\n",
      "\n",
      "Classification Report for Weighted KNN (K=10):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.79      0.91      0.85       381\n",
      "        >50K       0.44      0.22      0.29       119\n",
      "\n",
      "    accuracy                           0.75       500\n",
      "   macro avg       0.61      0.57      0.57       500\n",
      "weighted avg       0.71      0.75      0.71       500\n",
      "\n",
      "Classification Report for Majority KNN (K=10):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.78      0.99      0.87       381\n",
      "        >50K       0.86      0.10      0.18       119\n",
      "\n",
      "    accuracy                           0.78       500\n",
      "   macro avg       0.82      0.55      0.53       500\n",
      "weighted avg       0.80      0.78      0.71       500\n",
      "\n",
      "Classification Report for Weighted KNN (K=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.10      0.14      0.12        14\n",
      "          A+       0.11      0.12      0.12         8\n",
      "           B       0.10      0.14      0.12        21\n",
      "           C       0.23      0.39      0.29        23\n",
      "           D       0.33      0.23      0.27        40\n",
      "           F       0.50      0.12      0.20        24\n",
      "\n",
      "    accuracy                           0.21       130\n",
      "   macro avg       0.23      0.19      0.19       130\n",
      "weighted avg       0.27      0.21      0.21       130\n",
      "\n",
      "Classification Report for Majority KNN (K=1):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.10      0.14      0.12        14\n",
      "          A+       0.11      0.12      0.12         8\n",
      "           B       0.10      0.14      0.12        21\n",
      "           C       0.23      0.39      0.29        23\n",
      "           D       0.33      0.23      0.27        40\n",
      "           F       0.50      0.12      0.20        24\n",
      "\n",
      "    accuracy                           0.21       130\n",
      "   macro avg       0.23      0.19      0.19       130\n",
      "weighted avg       0.27      0.21      0.21       130\n",
      "\n",
      "Classification Report for Weighted KNN (K=5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.12      0.14      0.13        14\n",
      "          A+       0.00      0.00      0.00         8\n",
      "           B       0.13      0.19      0.15        21\n",
      "           C       0.21      0.35      0.26        23\n",
      "           D       0.37      0.33      0.35        40\n",
      "           F       0.50      0.17      0.25        24\n",
      "\n",
      "    accuracy                           0.24       130\n",
      "   macro avg       0.22      0.20      0.19       130\n",
      "weighted avg       0.28      0.24      0.24       130\n",
      "\n",
      "Classification Report for Majority KNN (K=5):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.17      0.29      0.21        14\n",
      "          A+       0.00      0.00      0.00         8\n",
      "           B       0.17      0.29      0.21        21\n",
      "           C       0.21      0.30      0.25        23\n",
      "           D       0.40      0.30      0.34        40\n",
      "           F       0.57      0.17      0.26        24\n",
      "\n",
      "    accuracy                           0.25       130\n",
      "   macro avg       0.25      0.22      0.21       130\n",
      "weighted avg       0.31      0.25      0.25       130\n",
      "\n",
      "Classification Report for Weighted KNN (K=10):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.10      0.07      0.08        14\n",
      "          A+       0.00      0.00      0.00         8\n",
      "           B       0.10      0.10      0.10        21\n",
      "           C       0.25      0.52      0.34        23\n",
      "           D       0.40      0.45      0.42        40\n",
      "           F       0.43      0.12      0.19        24\n",
      "\n",
      "    accuracy                           0.28       130\n",
      "   macro avg       0.21      0.21      0.19       130\n",
      "weighted avg       0.27      0.28      0.25       130\n",
      "\n",
      "Classification Report for Majority KNN (K=10):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.13      0.14      0.14        14\n",
      "          A+       0.00      0.00      0.00         8\n",
      "           B       0.15      0.19      0.17        21\n",
      "           C       0.21      0.39      0.27        23\n",
      "           D       0.42      0.42      0.42        40\n",
      "           F       0.50      0.12      0.20        24\n",
      "\n",
      "    accuracy                           0.27       130\n",
      "   macro avg       0.24      0.21      0.20       130\n",
      "weighted avg       0.30      0.27      0.26       130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def knn_classifier_report(num_dataset):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(num_dataset.iloc[:,:-1], num_dataset.iloc[:,-1], test_size=0.2, random_state=1)\n",
    "\n",
    "    for k in [1, 5, 10]:\n",
    "\n",
    "        knn_weighted = KNeighborsClassifier(n_neighbors=k, weights='distance', metric='euclidean')\n",
    "        knn_weighted.fit(X_train.values, y_train)\n",
    "        y_pred_weighted = knn_weighted.predict(X_test.values)\n",
    "\n",
    "        knn_majority = KNeighborsClassifier(n_neighbors=k, weights='uniform', metric='euclidean')\n",
    "        knn_majority.fit(X_train.values, y_train)\n",
    "        y_pred_majority = knn_majority.predict(X_test.values)\n",
    "\n",
    "        print(f\"Classification Report for Weighted KNN (K={k}):\")\n",
    "        print(classification_report(y_test, y_pred_weighted, zero_division=0))\n",
    "\n",
    "        print(f\"Classification Report for Majority KNN (K={k}):\")\n",
    "        print(classification_report(y_test, y_pred_majority, zero_division=0))\n",
    "\n",
    "knn_classifier_report(adult_num_dataset)\n",
    "knn_classifier_report(student_num_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd38e99-7563-4d62-a079-d4cac558c642",
   "metadata": {},
   "source": [
    "Adult Dataset:\n",
    "\n",
    "K=1: Both Weighted KNN and Majority KNN have the same accuracy (0.7), therefore no difference with the voting strategy.\n",
    "\n",
    "K=5: Weighted KNN has a slightly higher accuracy (0.78) compared to Majority KNN (0.73).\n",
    "\n",
    "K=10: Weighted KNN has a slightly higher accuracy (0.79) compared to Majority KNN (0.78)\n",
    "\n",
    "The majority KNN tends to perform slightly better in terms of accuracy across different values of K. This suggests that the majority voting strategy is more suitable for this dataset, where the majority class (\"<=50K\") is more prevalent.\n",
    "\n",
    "As the number of nearest neighbors (K) increases, both models show a trend of increasing accuracy, with the majority KNN consistently having a slight advantage in accuracy. This aligns with the concept that increasing K tends to reduce the impact of noise on predictions.\n",
    "\n",
    "\n",
    "Student Dataset:\n",
    "\n",
    "K=1: Both Weighted KNN and Majority KNN have the same accuracy (0.26), therefore no difference with the voting strategy.\n",
    "\n",
    "K=5: Weighted KNN has a slightly higher accuracy (0.28) compared to Majority KNN (0.27)\n",
    "\n",
    "K=10: Weighted KNN has a slightly higher accuracy (0.29) compared to Majority KNN (0.28)\n",
    "\n",
    "Both weighted and majority KNN models have similar accuracy values, with KNN models with inverse distance weighting performing slightly better. This indicates that the voting strategy has less of an impact on the performance with both models struggle to achieve high accuracy, which suggests that the dataset's characteristics pose challenges for both voting strategies.\n",
    "\n",
    "Similar to the Adult Dataset, increasing the number of nearest neighbors generally leads to improved accuracy for both models. However, the improvements are relatively small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enormous-swing",
   "metadata": {},
   "source": [
    "**C)** How would standardisation impact the performance of your KNN models and Gaussian Naive Bayes model for the Adult dataset? **[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "static-grace",
   "metadata": {},
   "source": [
    "K-Nearest Neighbor (KNN) Models:\n",
    "Standardization can have a significant impact on the performance of KNN models, particularly when using distance-based metrics like Euclidean distance. Standardization rescales the features so that they have a mean of zero and a standard deviation of one. This transformation ensures that all features are on the same scale, preventing features with larger magnitudes from dominating the distance calculations. The impact of standardization on KNN with majority voting might be less pronounced cas it does not consider the distance between neighbors. However, standardization can still help ensure that features contribute equally to the classification process, potentially leading to a more balanced and accurate model.\n",
    "\n",
    "\n",
    "Gaussian Naive Bayes (GNB) Model:\n",
    "GNB models assume that features follow a Gaussian distribution and standardization could help align the features with this Gaussian distribution assumption. This helps the GNB model perform better because it aligns with how the data is expected to be distributed.\n",
    "\n",
    "Therefore, standardization can have positive effects on the performance of KNN models and the GNB model for the Adult dataset. It can contribute to improved accuracy, precision, and recall by ensuring balanced feature scales and aligning with the assumptions of the algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2419db",
   "metadata": {},
   "source": [
    "## Question 5. Evaluation metrics [2 marks]\n",
    "\n",
    "**A)** Update the code in questions 2, 3, and 4 to compute the following metrics for the models listed below:\n",
    "\n",
    "- One-R \n",
    "- Gaussian NB \n",
    "- Categorical NB\n",
    "- 3-Nearest Neighbor model with Euclidean distance and majority voting \n",
    "\n",
    "Report their performance using the following two metrics\n",
    "- micro-averaged precision\n",
    "- macro-averaged precision \n",
    " \n",
    "Conversely, you can also choose to implement the same 10 rounds of train and test split (80% train, 20% test) as described in the questions 2,3, and 4 in the code block below and report the average scores for the micro-precision and macro-precision.\n",
    "\n",
    "**[0.5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "8ad0ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adult Dataset Evaluation results:\n",
      "Accuracy of One-R: 0.76\n",
      "Accuracy of GNB: 0.8\n",
      "Accuracy of CNB: 0.8\n",
      "Accuracy of KNN(3): 0.76\n",
      "Micro-p of One-R: 0.76\n",
      "Micro-p of GNB: 0.8\n",
      "Micro-p of CNB: 0.8\n",
      "Micro-p of KNN(3): 0.76\n",
      "Macro-p of One-R: 0.6\n",
      "Macro-p of GNB: 0.74\n",
      "Macro-p of CNB: 0.73\n",
      "Macro-p of KNN(3): 0.64\n",
      "\n",
      "\n",
      "Student Dataset Evaluation results:\n",
      "Accuracy of One-R: 0.31\n",
      "Accuracy of GNB: 0.17\n",
      "Accuracy of CNB: 0.34\n",
      "Accuracy of KNN(3): 0.23\n",
      "Micro-p of One-R: 0.31\n",
      "Micro-p of GNB: 0.17\n",
      "Micro-p of CNB: 0.34\n",
      "Micro-p of KNN(3): 0.23\n",
      "Macro-p of One-R: 0.1\n",
      "Macro-p of GNB: 0.21\n",
      "Macro-p of CNB: 0.27\n",
      "Macro-p of KNN(3): 0.26\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, make_scorer\n",
    "\n",
    "def compare_eval(num_dataset, cat_dataset):\n",
    "\n",
    "    OneR_microP_1 = []\n",
    "    GNB_microP_1 = []\n",
    "    CNB_microP_1 = []\n",
    "    KNN3_microP_1_majority = []\n",
    "\n",
    "    OneR_macroP_1 = []\n",
    "    GNB_macroP_1 = []\n",
    "    CNB_macroP_1 = []\n",
    "    KNN3_macroP_1_majority = []\n",
    "\n",
    "    OneR_Acc_1 = []\n",
    "    GNB_Acc_1 = []\n",
    "    CNB_Acc_1 = []\n",
    "    KNN3_Acc_1_majority = []\n",
    "    \n",
    "    for i in range(10):\n",
    "\n",
    "        X = cat_dataset.drop('label', axis=1)\n",
    "        y = cat_dataset['label']\n",
    "\n",
    "        categorical_columns = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "        encoder = OrdinalEncoder()\n",
    "        X_encoded = X.copy()\n",
    "        X_encoded[categorical_columns] = encoder.fit_transform(X_encoded[categorical_columns])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=i)\n",
    "        X_train_num, X_test_num, y_train_num, y_test_num = train_test_split(\n",
    "            num_dataset.iloc[:, :-1], num_dataset['label'], test_size=0.2, random_state=i)\n",
    "\n",
    "        # One-R\n",
    "        best_feature, best_rules = one_r_train(X_train_num, y_train_num)\n",
    "        majority_class = Counter(y_train).most_common(1)[0][0]\n",
    "        one_r_predictions = one_r_predict(X_test_num, best_feature, best_rules, majority_class)\n",
    "        one_r_micro_precision = precision_score(y_test_num, one_r_predictions, average='micro', zero_division=0)\n",
    "        one_r_macro_precision = precision_score(y_test_num, one_r_predictions, average='macro', zero_division=0)\n",
    "        OneR_Acc_1.append(accuracy_score(y_test, one_r_predictions))\n",
    "        OneR_microP_1.append(one_r_micro_precision)\n",
    "        OneR_macroP_1.append(one_r_macro_precision)\n",
    "\n",
    "        # Gaussian Naive Bayes\n",
    "        gnb = GaussianNB()\n",
    "        gnb.fit(X_train_num, y_train_num)\n",
    "        y_pred = gnb.predict(X_test_num)\n",
    "        gnb_micro_precision = precision_score(y_test_num, y_pred, average='micro', zero_division=0)\n",
    "        gnb_macro_precision = precision_score(y_test_num, y_pred, average='macro', zero_division=0)\n",
    "        GNB_Acc_1.append(accuracy_score(y_test, y_pred))\n",
    "        GNB_microP_1.append(gnb_micro_precision)\n",
    "        GNB_macroP_1.append(gnb_macro_precision)\n",
    "\n",
    "        # Categorical Naive Bayes\n",
    "        min_categories = X_encoded.nunique()\n",
    "        cnb = CategoricalNB(min_categories=min_categories)\n",
    "        cnb.fit(X_train, y_train)\n",
    "        y_pred = cnb.predict(X_test)\n",
    "        cnb_micro_precision = precision_score(y_test, y_pred, average='micro', zero_division=0)\n",
    "        cnb_macro_precision = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "        CNB_Acc_1.append(accuracy_score(y_test, y_pred))\n",
    "        CNB_microP_1.append(cnb_micro_precision)\n",
    "        CNB_macroP_1.append(cnb_macro_precision)\n",
    "\n",
    "        # 3-Nearest Neighbor model\n",
    "        knn_model = KNeighborsClassifier(n_neighbors=3, weights='uniform', metric='euclidean')\n",
    "        knn_model.fit(X_train_num.values, y_train_num)\n",
    "        y_pred_knn = knn_model.predict(X_test_num.values)\n",
    "        knn_micro_precision = precision_score(y_test, y_pred_knn, average='micro', zero_division=0)\n",
    "        knn_macro_precision = precision_score(y_test, y_pred_knn, average='macro', zero_division=0)\n",
    "        KNN3_Acc_1_majority.append(accuracy_score(y_test, y_pred_knn))\n",
    "        KNN3_microP_1_majority.append(knn_micro_precision)\n",
    "        KNN3_macroP_1_majority.append(knn_macro_precision)\n",
    "\n",
    "\n",
    "    print(\"Accuracy of One-R:\", np.mean(OneR_Acc_1).round(2))\n",
    "    print(\"Accuracy of GNB:\", np.mean(GNB_Acc_1).round(2))\n",
    "    print(\"Accuracy of CNB:\", np.mean(CNB_Acc_1).round(2)) \n",
    "    print(\"Accuracy of KNN(3):\", np.mean(KNN3_Acc_1_majority).round(2))\n",
    "\n",
    "    print(\"Micro-p of One-R:\", np.mean(OneR_microP_1).round(2))\n",
    "    print(\"Micro-p of GNB:\", np.mean(GNB_microP_1).round(2))\n",
    "    print(\"Micro-p of CNB:\", np.mean(CNB_microP_1).round(2)) \n",
    "    print(\"Micro-p of KNN(3):\", np.mean(KNN3_microP_1_majority).round(2))\n",
    "\n",
    "    print(\"Macro-p of One-R:\", np.mean(OneR_macroP_1).round(2))\n",
    "    print(\"Macro-p of GNB:\", np.mean(GNB_macroP_1).round(2))\n",
    "    print(\"Macro-p of CNB:\", np.mean(CNB_macroP_1).round(2)) \n",
    "    print(\"Macro-p of KNN(3):\", np.mean(KNN3_macroP_1_majority).round(2))\n",
    "    \n",
    "\n",
    "##Adult Dataset and Student Dataset results: \n",
    "\n",
    "print(\"Adult Dataset Evaluation results:\")\n",
    "compare_eval(adult_num_dataset,adult_cat_dataset)\n",
    "print(\"\\n\")\n",
    "print(\"Student Dataset Evaluation results:\")\n",
    "compare_eval(student_num_dataset,student_cat_dataset)    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f81f752",
   "metadata": {},
   "source": [
    "**B)** Compare the average accuracy vs. macro-average and micro-average precision for the two datasets. Explain which evaluation measurement would be most appropriate for each dataset **[1.5 mark]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-sleep",
   "metadata": {},
   "source": [
    "Adult Dataset:\n",
    "\n",
    "The accuracy values for the classifiers (One-R, GNB, CNB, KNN(3)) range from 0.76 to 0.8, indicating relatively high accuracy across the models. The micro-averaged precision values are consistent with the accuracy values, ranging from 0.76 to 0.8. The macro-averaged precision values are lower than the micro-averaged precision and accuracy values, ranging from 0.6 to 0.74.\n",
    "\n",
    "Appropriate Evaluation Metric: Micro-averaged Precision\n",
    "\n",
    "Since micro-averaged precision considers both true positives and false positives across all classes and then computes a single precision value, it aligns well with the balanced nature of the dataset and the high accuracy values. This metric provides an overall understanding of how well the models are performing in terms of precision.\n",
    "\n",
    "Student Dataset:\n",
    "\n",
    "The accuracy values for the classifiers range from 0.17 to 0.34, indicating relatively low accuracy across the models. The micro-averaged precision values are consistent with the accuracy values, ranging from 0.17 to 0.34. The macro-averaged precision values are also relatively low, ranging from 0.1 to 0.27.\n",
    "\n",
    "Appropriate Evaluation Metric: Macro-averaged Precision\n",
    "\n",
    "Macro-averaged precision calculates the precision for each class individually and then takes their average, giving equal weight to each class. Since the dataset is imbalanced(due to relatively low accuracy and micro-averaged precision) and there are variations in class distributions, using macro-averaged precision helps prevent the dominant class from overshadowing the performance of minority classes. This metric provides insights into how well the models are performing across different classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-pollution",
   "metadata": {},
   "source": [
    "## Question 6. Ethics and implications in practice [4 marks]\n",
    "\n",
    "The Categorical Naive Bayes classifier you developed in this assignment for the student dataset could for example be used to classify college applicants into admitted vs not-admitted depending on their predicted grade in the Student dataset.\n",
    "\n",
    "**A)** Discuss ethical problems which might arise in this application and lead to unfair treatment of the applicants. Ground your discussion in the set of features provided in the student data set.**[1 marks]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-package",
   "metadata": {},
   "source": [
    "Some ethical problems that may arise in this application and lead to unfair treatment of applicants are:\n",
    "\n",
    "##### Gender Bias: \n",
    "The 'sex' feature can lead to gender bias, with the model incorrectly associating a certain gender with having a higher or lower grade.\n",
    "\n",
    "\n",
    "##### Socioeconomic Bias:\n",
    "The 'Medu', 'Fedu', 'Mjob', and 'Fjob' features can lead to a socioeconomic bias where students from families with more educated parents and/or well reputated and high paying jobs could be favored, without considering the challenges faced by students who are not from these backgrounds.\n",
    "\n",
    "\n",
    "##### Educational Access Bias:\n",
    "The 'internet' feature can correlate to access to educational resources, and if the model incorrectly values this attibute, this could result in an unfair treatment to students lacking access to the resource.\n",
    "\n",
    "\n",
    "##### Health and Personal Circumstances Bias:\n",
    "Features like 'health' and 'absences', which can often be as a result of personal circumstances, and the model can inaccurately treat these features as a measure of the students commitment, and academic potential, it could lead to unfair decisions for students with health conditions and absenses due to personal circumstances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classified-hudson",
   "metadata": {},
   "source": [
    "**B)** Remove all ethically problematic features from the data set (use your own judgment), and train your Naive Bayes classifiers on the resulting data set. How does the performance change in comparison to the full classifier ( consider accuracy and micro-average precision)?**[2 marks]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "16654485-1893-4780-a9a7-e5939277901d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier without ethically problematic features:\n",
      "Accuracy of GNB: 0.17\n",
      "Accuracy of BNB: 0.32\n",
      "Accuracy of CNB: 0.34\n"
     ]
    }
   ],
   "source": [
    "unethical_features = ['sex', 'Medu', 'Fedu', 'internet', 'romantic', 'health', 'famsize', 'Pstatus', 'guardian', 'famrel',\n",
    "                      'absences', 'Mjob','guardian', 'famrel']\n",
    "\n",
    "filtered_student_cat_dataset = student_cat_dataset.drop(columns=unethical_features)\n",
    "\n",
    "print(\"Classifier without ethically problematic features:\")\n",
    "NB_models(student_num_dataset, filtered_student_cat_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "872a3d5c-f59d-418c-8d19-31702ea6b0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "Removing the ethically problematic features does not affect the accuracies as compared to the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adequate-greeting",
   "metadata": {},
   "source": [
    "**C)** The approach to fairness we have adopted is called “fairness through unawareness”, where we simply deleted any questionable features from our data. Is removing all problematic features as done in part (b) guarantee a fair classifier? Explain Why or Why not?**[1 marks]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handed-chester",
   "metadata": {},
   "source": [
    "\n",
    "Removing all problematic features does not guarantee a fair classifier. While removing problematic features can mitigate some potential sources of bias and discrimination, it does not address the underlying issues that might still exist in the remaining features. \n",
    "\n",
    "Some reasons why removing problematic features might not guarantee fairness:\n",
    "\n",
    "Proxy Features: Sometimes, problematic features can be proxies for other characteristics that are still present in the dataset. Even if the direct features are removed, the information they were capturing might still be present in other correlated features.\n",
    "\n",
    "Hidden Bias: Bias can still be present in the remaining features that were not identified as problematic. The model can learn hidden biases from the data itself, even if those biases were not explicitly represented by the removed features.\n",
    "\n",
    "In order to achieve fairness, more advanced techniques that specifically focus on identifying and mitigating bias in the data and model like re-sampling, re-weighting, adversarial training, and fairness-aware loss functions should be used. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55940e72-3414-4dab-be69-96a579354617",
   "metadata": {},
   "source": [
    "# Authorship Declaration:\n",
    "\n",
    "   (1) I certify that the program contained in this submission is completely\n",
    "   my own individual work, except where explicitly noted by comments that\n",
    "   provide details otherwise.  I understand that work that has been developed\n",
    "   by another student, or by me in collaboration with other students,\n",
    "   or by non-students as a result of request, solicitation, or payment,\n",
    "   may not be submitted for assessment in this subject.  I understand that\n",
    "   submitting for assessment work developed by or in collaboration with\n",
    "   other students or non-students constitutes Academic Misconduct, and\n",
    "   may be penalized by mark deductions, or by other penalties determined\n",
    "   via the University of Melbourne Academic Honesty Policy, as described\n",
    "   at https://academicintegrity.unimelb.edu.au.\n",
    "\n",
    "   (2) I also certify that I have not provided a copy of this work in either\n",
    "   softcopy or hardcopy or any other form to any other student, and nor will\n",
    "   I do so until after the marks are released. I understand that providing\n",
    "   my work to other students, regardless of my intention or any undertakings\n",
    "   made to me by that other student, is also Academic Misconduct.\n",
    "\n",
    "   (3) I further understand that providing a copy of the assignment\n",
    "   specification to any form of code authoring or assignment tutoring\n",
    "   service, or drawing the attention of others to such services and code\n",
    "   that may have been made available via such a service, may be regarded\n",
    "   as Student General Misconduct (interfering with the teaching activities\n",
    "   of the University and/or inciting others to commit Academic Misconduct).\n",
    "   I understand that an allegation of Student General Misconduct may arise\n",
    "   regardless of whether or not I personally make use of such solutions\n",
    "   or sought benefit from such actions.\n",
    "\n",
    "   <b>Signed by</b>: Kian Dsouza\n",
    "   \n",
    "   <b>Dated</b>: 01-09-2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
