{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# some toy data (for debug)\n",
    "x_train = [[2,1,1,1], #0\n",
    "     [0,2,0,1], #1\n",
    "     [1,1,0,1], #0\n",
    "     [1,0,0,0], #1\n",
    "     [2,2,0,1]] #0\n",
    "y_train = [0,1,0,1,0]\n",
    "\n",
    "\n",
    "x_test = [[2,2,1,1], #0\n",
    "     [0,2,0,0]] #1\n",
    "y_test = [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE of prior p(y=i) = n(i)/N -- not normalizing here, just counting numerator\n",
    "def p_y(y):\n",
    "    class_priors = [0]*len(set(y))\n",
    "    for c in y:\n",
    "        class_priors[c]+=1    \n",
    "    return class_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLE of likelihood p(x=j|y=i) = n(i,j)/n(i)\n",
    "# Default epsilon smoothing with e=0.001\n",
    "\n",
    "def p_xy(x,y,py,epsilon=0.001):\n",
    "    \n",
    "    # init dict (over classes) of dict (over features) of dict (over value counts)\n",
    "    outdict = {c:{} for c in y}\n",
    "    for d in outdict.keys():\n",
    "        for f in range(len(x[0])):\n",
    "            outdict[d][f]={}\n",
    "            rng = set([i[f] for i in x])\n",
    "            outdict[d][f] = {v:0 for v in rng}\n",
    "    \n",
    "    #for k, v in outdict.items():\n",
    "    #    print(\"{}\\t{}\".format(k,v))\n",
    "    #print()\n",
    "        \n",
    "    # fill dict with counts\n",
    "    for idx,_ in enumerate(x):\n",
    "        for fidx, _ in enumerate(x[idx]):\n",
    "            outdict[y[idx]][fidx][x[idx][fidx]]+=1\n",
    "\n",
    "    #for k, v in outdict.items():\n",
    "        #print(\"{}\\t{}\".format(k,v))\n",
    "    #print()\n",
    "            \n",
    "    # normalize, or fill in epsilons as needed\n",
    "    for cl in outdict.keys():\n",
    "        for f in outdict[cl].keys():\n",
    "            for val in outdict[cl][f]:\n",
    "                if outdict[cl][f][val] > 0:\n",
    "                    outdict[cl][f][val] = outdict[cl][f][val] / py[cl]\n",
    "                else:\n",
    "                    outdict[cl][f][val] = epsilon\n",
    "            \n",
    "    #for k, v in outdict.items():\n",
    "        #print(\"{}\\t{}\".format(k,v))\n",
    "    return outdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def predict(x, pc, pxc, epsilon=0.001):\n",
    "    # sums up prior and independent likelihood terms\n",
    "    # (fills in epsilons for unseen feature values during training)\n",
    "    class_probs = []\n",
    "    for y in range(len(pc)):\n",
    "        class_prob=pc[y]/sum(pc)\n",
    "        #print(class_prob)\n",
    "        for fidx, f in enumerate(x):\n",
    "            if f in pxc[y][fidx]:\n",
    "                class_prob = class_prob * pxc[y][fidx][f]\n",
    "            else:\n",
    "                class_prob = class_prob * epsilon\n",
    "            #print(class_prob)\n",
    "        class_probs.append(class_prob)\n",
    "    return class_probs, np.argmax([class_probs])\n",
    "        \n",
    "    \n",
    "    \n",
    "def log_predict(x, pc, pxc, epsilon=0.001):\n",
    "    # sums up prior and independent likelihood terms\n",
    "    # (fills in epsilons for unseen feature values during training)\n",
    "    class_probs = []\n",
    "    for y in range(len(pc)):\n",
    "        class_prob=math.log(pc[y]/sum(pc))\n",
    "        #print(class_prob)\n",
    "        for fidx, f in enumerate(x):\n",
    "            if f in pxc[y][fidx]:\n",
    "                class_prob = class_prob + math.log(pxc[y][fidx][f])\n",
    "            else:\n",
    "                class_prob = class_prob + math.log(epsilon)\n",
    "            #print(class_prob)\n",
    "        class_probs.append(class_prob)\n",
    "    return class_probs, np.argmax([class_probs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "py = p_y(y_train)\n",
    "pxy = p_xy(x_train,y_train,py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([0.04444444444444443, 1.0000000000000001e-07], 0)\n",
      "([-3.113515309210375, -16.118095650958317], 0)\n",
      "([1.333333333333333e-07, 0.05], 1)\n",
      "([-15.830413578506537, -2.995732273553991], 1)\n"
     ]
    }
   ],
   "source": [
    "for x in x_test:\n",
    "    print(predict(x, py, pxy))\n",
    "    print(log_predict(x, py, pxy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "def preprocess(filepath, verbose=True):\n",
    "    \n",
    "    data = pandas.read_csv(filepath)\n",
    "    data = data.sample(frac=1, random_state=1).reset_index(drop=True) # shuffle data\n",
    "\n",
    "    # map string values to integers\n",
    "    for c in list(data):\n",
    "        vals = sorted(set([v for v in data[c].values]))\n",
    "        vals_dict = dict(zip(vals, range(len(vals))))\n",
    "        data[c] = data[c].map(lambda s: vals_dict.get(s) if s in vals_dict else s)\n",
    "\n",
    "    data_y = data.iloc[:,-1]\n",
    "    data_x = data.iloc[:,:-1]\n",
    "\n",
    "    assert data_y.shape[0] == data_x.shape[0]\n",
    "\n",
    "    print(\"N: {}    Feats: {}    Classes: {}    Balance: {}\".format(data_x.shape[0], \n",
    "                                                     data_x.shape[1], \n",
    "                                                     len(set(list(data_y))),\n",
    "                                                     [(i,list(data_y).count(i)) for i in set(list(data_y))]))\n",
    "    return data_x, data_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(x,y,trainportion=0.9):\n",
    "    ntrain = int(float(len(x))*trainportion)\n",
    "    print(\"{} training examples\".format(ntrain))\n",
    "    return x[:ntrain].reset_index(drop=True), y[:ntrain].reset_index(drop=True), x[ntrain:].reset_index(drop=True), y[ntrain:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def prf1(pred,true):\n",
    "    return metrics.precision_recall_fscore_support(true, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "portugese student data\n",
      "N: 649    Feats: 29    Classes: 6    Balance: [(0, 65), (1, 17), (2, 112), (3, 154), (4, 201), (5, 100)]\n",
      "519 training examples\n",
      "\n",
      "evaluation on training data (bad!)\n",
      "accuracy: 0.5202312138728323\n",
      "pr [0.39285714 0.6        0.5        0.5        0.53216374 0.62666667]\n",
      "re [0.44       0.69230769 0.45054945 0.49586777 0.5617284  0.57317073]\n",
      "f1 [0.41509434 0.64285714 0.47398844 0.49792531 0.54654655 0.59872611]\n",
      "\n",
      "evaluation on test data (good!)\n",
      "accuracy: 0.36153846153846153\n",
      "pr [0.27272727 0.         0.2        0.37931034 0.44680851 0.38095238]\n",
      "re [0.2        0.         0.19047619 0.33333333 0.53846154 0.44444444]\n",
      "f1 [0.23076923 0.         0.19512195 0.35483871 0.48837209 0.41025641]\n"
     ]
    }
   ],
   "source": [
    "# load data data\n",
    "print(\"\\nportugese student data\")\n",
    "bc_x, bc_y = preprocess('student-por_assignment.csv')\n",
    "\n",
    "# split data and re-format to list\n",
    "x_tr, y_tr, x_te, y_te = split_data(bc_x, bc_y, 0.8)\n",
    "bx_train = x_tr.values.tolist()\n",
    "by_train = y_tr.values.tolist()\n",
    "\n",
    "bx_test = x_te.values.tolist()\n",
    "by_test = y_te.values.tolist()\n",
    "\n",
    "# estimate\n",
    "py = p_y(by_train)\n",
    "pxy = p_xy(bx_train,by_train,py)\n",
    "\n",
    "# predict on train\n",
    "print(\"\\nevaluation on training data (bad!)\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "for idx, i in enumerate(bx_train):\n",
    "    prediction = log_predict(i,py, pxy)[1]\n",
    "    correct = correct + int(prediction==by_train[idx])\n",
    "    preds.append(prediction)\n",
    "precf1 = prf1(preds, by_train)\n",
    "\n",
    "print(\"accuracy: {}\\npr {}\\nre {}\\nf1 {}\".format(correct / len(bx_train),\n",
    "                                                precf1[0],\n",
    "                                                precf1[1],\n",
    "                                                precf1[2]))\n",
    "\n",
    "# predict on test\n",
    "print(\"\\nevaluation on test data (good!)\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "for idx, i in enumerate(bx_test):\n",
    "    prediction = log_predict(i,py, pxy)[1]\n",
    "    correct = correct + int(prediction==by_test[idx])\n",
    "    preds.append(prediction)\n",
    "precf1 = prf1(preds, by_test)\n",
    "\n",
    "print(\"accuracy: {}\\npr {}\\nre {}\\nf1 {}\".format(correct / len(bx_test),\n",
    "                                                precf1[0],\n",
    "                                                precf1[1],\n",
    "                                                precf1[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the maths data\n",
    "\n",
    "# print(\"\\nmaths student data\")\n",
    "# bc_mat, bc_mat = preprocess('student-mat_assignment.csv')\n",
    "\n",
    "# split data and re-format to list\n",
    "# # bx_mat = x_tr.values.tolist()\n",
    "# by_mat = y_tr.values.tolist()\n",
    "\n",
    "\n",
    "# predict on maths\n",
    "# print(\"\\nevaluation on maths data (train on portugese)\")\n",
    "\n",
    "# correct = 0\n",
    "# preds = []\n",
    "# # for idx, i in enumerate(bx_mat):\n",
    "#     prediction = predict(i,py, pxy)[1]\n",
    "#     correct = correct + int(prediction==by_mat[idx])\n",
    "#     preds.append(prediction)\n",
    "# precf1 = prf1(preds, by_mat)\n",
    "\n",
    "# print(\"accuracy: {}\\npr {}\\nre {}\\nf1 {}\".format(correct / len(bx_mat),\n",
    "#                                                 precf1[0],\n",
    "#                                                 precf1[1],\n",
    "#                                                 precf1[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "portugese student data\n",
      "N: 649    Feats: 6    Classes: 6    Balance: [(0, 65), (1, 17), (2, 112), (3, 154), (4, 201), (5, 100)]\n",
      "519 training examples\n",
      "\n",
      "evaluation on training data (bad!)\n",
      "accuracy: 0.4258188824662813\n",
      "pr [0.33333333 1.         0.33333333 0.38709677 0.42857143 0.63076923]\n",
      "re [0.02       0.07692308 0.27472527 0.59504132 0.5        0.5       ]\n",
      "f1 [0.03773585 0.14285714 0.30120482 0.46905537 0.46153846 0.55782313]\n",
      "\n",
      "evaluation on test data (good!)\n",
      "accuracy: 0.35384615384615387\n",
      "pr [0.         0.         0.25       0.33333333 0.38888889 0.42105263]\n",
      "re [0.         0.         0.19047619 0.39393939 0.53846154 0.44444444]\n",
      "f1 [0.         0.         0.21621622 0.36111111 0.4516129  0.43243243]\n"
     ]
    }
   ],
   "source": [
    "# SAFE version of the data set (conservatively...)\n",
    "# load portugese data\n",
    "print(\"\\nportugese student data\")\n",
    "bc_x, bc_y = preprocess('student-por_safe2.csv')\n",
    "\n",
    "# split data and re-format to list\n",
    "x_tr, y_tr, x_te, y_te = split_data(bc_x, bc_y, 0.8)\n",
    "bx_train = x_tr.values.tolist()\n",
    "by_train = y_tr.values.tolist()\n",
    "\n",
    "bx_test = x_te.values.tolist()\n",
    "by_test = y_te.values.tolist()\n",
    "\n",
    "# estimate\n",
    "py = p_y(by_train)\n",
    "pxy = p_xy(bx_train,by_train,py)\n",
    "\n",
    "# predict on train\n",
    "print(\"\\nevaluation on training data (bad!)\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "for idx, i in enumerate(bx_train):\n",
    "    prediction = log_predict(i,py, pxy)[1]\n",
    "    correct = correct + int(prediction==by_train[idx])\n",
    "    preds.append(prediction)\n",
    "precf1 = prf1(preds, by_train)\n",
    "\n",
    "print(\"accuracy: {}\\npr {}\\nre {}\\nf1 {}\".format(correct / len(bx_train),\n",
    "                                                precf1[0],\n",
    "                                                precf1[1],\n",
    "                                                precf1[2]))\n",
    "\n",
    "# predict on test\n",
    "print(\"\\nevaluation on test data (good!)\")\n",
    "\n",
    "correct = 0\n",
    "preds = []\n",
    "for idx, i in enumerate(bx_test):\n",
    "    prediction = log_predict(i,py, pxy)[1]\n",
    "    correct = correct + int(prediction==by_test[idx])\n",
    "    preds.append(prediction)\n",
    "precf1 = prf1(preds, by_test)\n",
    "\n",
    "print(\"accuracy: {}\\npr {}\\nre {}\\nf1 {}\".format(correct / len(bx_test),\n",
    "                                                precf1[0],\n",
    "                                                precf1[1],\n",
    "                                                precf1[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Qusetion 5**\n",
    "\n",
    "- We don't want to use attributes like sex or education of mother to judge an applicants' ability. This should happen purely based on objective performance / attitude features (see next point)\n",
    "- I used attributes 1,11,14,15,21,30: school, reason, studytime, failures, higher, absences\n",
    "- The test performance hardly changes at all, for some classes it even goes up (the fit of the training data decreases, actually -- so: less *overfitting*)\n",
    "- why deleting features isn't enough? Because sensitive features (e.g., sex) correlate with others (e.g., studytime if traditionally we assume that girls had to help out more in the household, thus less time to study)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
